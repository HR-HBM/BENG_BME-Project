{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYi1vF1RZglqTYnRn7Qhtd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HR-HBM/BENG_BME-Project/blob/main/ME_Model_with_CLAHE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkB7Tw0-DJuJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "TRAIN_DIR = \"C:/Users/HR/Downloads/mbrset-a-mobile-brazilian-retinal-dataset-1.0/mbrset-a-mobile-brazilian-retinal-dataset-1.0/MaculaEdema/train\"\n",
        "TEST_DIR = \"C:/Users/HR/Downloads/mbrset-a-mobile-brazilian-retinal-dataset-1.0/mbrset-a-mobile-brazilian-retinal-dataset-1.0/MaculaEdema/test\"\n",
        "VAL_DIR = \"C:/Users/HR/Downloads/mbrset-a-mobile-brazilian-retinal-dataset-1.0/mbrset-a-mobile-brazilian-retinal-dataset-1.0/MaculaEdema/validate\"\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(TRAIN_DIR, target_size=(224,224), batch_size=32, class_mode=\"categorical\")\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "val_set = train_datagen.flow_from_directory(VAL_DIR, target_size=(224, 224), batch_size=32, class_mode=\"categorical\")\n",
        "\n",
        "\n",
        "\n",
        "class CLAHEPreprocessingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, clip_limit=3.0, tile_grid_size=(8, 8), **kwargs):\n",
        "        super(CLAHEPreprocessingLayer, self).__init__(**kwargs)\n",
        "        self.clip_limit = clip_limit\n",
        "        self.tile_grid_size = tile_grid_size\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Use tf.numpy_function to apply CLAHE\n",
        "        output = tf.numpy_function(self.apply_clahe, [inputs], tf.float32)\n",
        "\n",
        "        # Explicitly set the shape to avoid unknown tensor shape issue\n",
        "        output.set_shape(inputs.shape)  # Ensure it matches (None, 224, 224, 3)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def apply_clahe(self, img_batch):\n",
        "        processed_batch = []\n",
        "        for img in img_batch:\n",
        "            img = np.array(img, dtype=np.uint8)\n",
        "\n",
        "            # Convert to grayscale\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "            # Apply CLAHE\n",
        "            clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=(8, 8))\n",
        "            enhanced = clahe.apply(gray)\n",
        "\n",
        "            # Convert back to 3-channel grayscale RGB\n",
        "            enhanced_rgb = cv2.merge([enhanced, enhanced, enhanced])\n",
        "\n",
        "            # Ensure output matches (224, 224, 3)\n",
        "            enhanced_rgb = cv2.resize(enhanced_rgb, (224, 224))\n",
        "            processed_batch.append(enhanced_rgb)\n",
        "\n",
        "        return np.array(processed_batch, dtype=np.float32)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape  # Ensures output matches input shape\n",
        "\n",
        "\n",
        "\n",
        "IMG_SHAPE = (224,224,3)\n",
        "base_model = tf.keras.applications.EfficientNetB1(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "\n",
        "  CLAHEPreprocessingLayer(),\n",
        "  tf.keras.layers.Rescaling(1./255, input_shape=IMG_SHAPE),  # Normalize the image within the model\n",
        "# Apply CLAHE\n",
        "  base_model,\n",
        "  tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001), #Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(x=train_set, validation_data=val_set, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "print(acc)\n",
        "print(val_acc)\n",
        "\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_range, acc, label=\"Training accuracy\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Validation accuracy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"Training and Validation Accuracy (with CLAHE)\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
        "plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title(\"Training and Validation Loss (with CLAHE)\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "model.save(\"C:/Users/HR/Downloads/mbrset-a-mobile-brazilian-retinal-dataset-1.0/mbrset-a-mobile-brazilian-retinal-dataset-1.0/MaculaEdema/MaculaEdema-wCLAHE.h5\")"
      ]
    }
  ]
}